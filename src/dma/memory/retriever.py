from dma.core import RetrievalStep, EntityQuery, EmbeddingQuery, RetrievalQuery, Retrieval
from dma.core import Conversation, Message, Role
from dma.core import Memory, MemoryResult, FeedbackType, MemoryFeedback
from dma.utils import cosine_similarity

from .graph import Neo4jMemory, GraphMemory, GraphResult
from .evaluator import Evaluation, MemoryRelevance
import math
import logging

from dma.config.dma_config import get_config

class Retriever:
    """A retriever is responsible for fetching relevant information from memory
    based on the queries generated by the pipeline.
    It uses the graph database and vector database to retrieve information.
    
    At also allows new memories to be added to the memory.
    """
    
    def __init__(self):
        self.config = get_config()
        
        self.graph_memory: GraphMemory = Neo4jMemory()
        
        
    def add_memory(self, memory: Memory) -> bool:
        """Add a single memory to the memory backend.
        
        Parameters
        ----------
        memory : Memory
            The memory object to add.
            
        Returns
        -------
        bool
            True if the memory was added successfully, False otherwise.
        """
        return self.graph_memory.add_memory(memory)

    def add_memory_batch(self, memories: list[Memory]) -> list[str]:
        """Add a batch of memories to the memory backend.
        
        Parameters
        ----------
        memories : list[Memory]
            The list of memory objects to add.

        Returns
        -------
        list[str]
            A list of IDs for the added memories.
        """
        return self.graph_memory.add_memory_batch(memories)

    def add_memory_series(self, memories: list[Memory]) -> bool:
        """Add a series of memories to the memory backend.
        Memories in a series are linked together in order.

        Parameters
        ----------
        memories : list[Memory]
            The list of memory objects to add.
        
        Returns
        -------
        bool
            True if the memories were added successfully, False otherwise.
        """
        return self.graph_memory.add_memory_series(memories)
    
    def retrieve(self, conversation: Conversation, query: RetrievalStep, previous_retrievals: Retrieval=None, top_k: int = 10) -> list[MemoryResult]:
        """Retrieve relevant memories based on the provided query.
        
        Parameters
        ----------
        conversation : Conversation
            The current conversation context.
        query : RetrievalStep
            The retrieval step containing the queries.
        top_k : int
            The number of top results to retrieve.
        
        Returns
        -------
        list[Memory]
            A list of retrieved memories.
        """
        blacklisted_memory_ids = set()
        if previous_retrievals is not None:
            for retrieval in previous_retrievals.steps:
                for result in retrieval.rejected_results:
                    blacklisted_memory_ids.add(result.memory.id)
                for result in retrieval.results:
                    blacklisted_memory_ids.add(result.memory.id)
        
        # 1. create list of entities in previous assistant messages, with count and recency
        entity_scales = self._calculate_entity_scores(conversation)
        
        entities, embeddings = self._collect_queries(query)
        # apply entity scales
        for entity in entities:
            if entity in entity_scales:
                entities[entity] = entities[entity] * entity_scales[entity]
               
        # 2. query and collect results
        TOP_K_ENTITIES = 10
        # limit to top k entities
        sorted_entities = sorted(entities.items(), key=lambda item: item[1], reverse=True)[:TOP_K_ENTITIES]
        sorted_entities = [entity for entity, _ in sorted_entities]
        
        TOP_K_QUERY_RESULTS = max(20, top_k*2)  # ensure we get enough results to rank from
        embedding_results = [self.graph_memory.query_memories_by_vector(embedding, top_k=TOP_K_QUERY_RESULTS) for embedding in embeddings]
        entity_results = self.graph_memory.query_memories_by_entities(sorted_entities, limit=TOP_K_QUERY_RESULTS)
        
        # r = []
        # while len(r) < top_k:
        #     for embed_list in embedding_results:
        #         if len(embed_list) > 0:
        #             r.append(embed_list.pop(0))
        #             if len(r) >= top_k:
        #                 break
                    
        # return r
        
        # MIN_ENTITY_SCORE = max(1.0, len(entities) / 4.0)
        MIN_SIMILARITY = 0.5
        
        all_memory_results = {}
        for entity_list in entity_results.values():
            for graph_result in entity_list:
                all_memory_results[graph_result.memory.id] = graph_result.memory
        for embed_list in embedding_results:
            for graph_result in embed_list:
                if graph_result.score < MIN_SIMILARITY:
                    continue
                all_memory_results[graph_result.memory.id] = graph_result.memory

        # to list
        all_memory_results = list(all_memory_results.values())
        
        # print(f"Initial n results before filtering: {len(all_memory_results)}")
        
        # filter out blacklisted memories
        all_memory_results = [mem for mem in all_memory_results if not mem.id in blacklisted_memory_ids]
        
        # print(f"n results after blacklisting: {len(all_memory_results)}")
            
                
        # 3. rerank results based on entity scales and other factors
        # make sure to prioritize embedding results if present
        ranked_results = self._rank_memories(all_memory_results, entities, embeddings)
        
        # print(f"Retriever: Ranked {len(ranked_results)} memories.")
        
        return ranked_results[:top_k]
    
        # disable expansion for now
        
        # use 60:40 split between direct results and context expanded results
        if top_k == None:
            top_k = len(ranked_results) / 0.6  # ensure we get all direct results if no top_k specified
        n_direct = int(top_k * 0.6)
        n_expanded = top_k - n_direct
        
        direct_results = ranked_results[:n_direct]
        

        
        # 4. expand query based on results and re-query if needed
        # possibilities: 
        # - get related entities from graph (calculated using co-mentions)
        # - get related memories (using graph links)
        # rerank based on:
        # - original_memory_score * distance_scale
        # - original_entity_score * entity_similarity_scale
        # - time decay if applicable
        
        blacklisted_memory_ids = blacklisted_memory_ids.union([res.memory.id for res in direct_results])
        
        # convert to list, since deep_relationship_traversal expects list
        blacklisted_memory_ids = list(blacklisted_memory_ids)
        
        # get context expanded results for top 3 direct results
        # TODO: consider using memories from previous retrieval steps as well for expansion
        # since these have already been deemed relevant
        # (or mix of previous retrievals and direct results)
        TOP_DIRECT_FOR_EXPANSION = 3
        all_expensions = []
        for direct_result in direct_results[:TOP_DIRECT_FOR_EXPANSION]:
            expansions = self.graph_memory.deep_relationship_traversal(
                direct_result.memory.id, 
                max_depth=4, 
                stop_k=n_expanded,
                blacklist_ids=blacklisted_memory_ids
                )
            all_expensions.append(expansions)
            
        # combine all iterations by summing up scores and then applying dropoff
        unique_expanded_memories = {}
        for expansions in all_expensions:
            for graph_result in expansions:
                if not graph_result.memory.id in unique_expanded_memories:
                    unique_expanded_memories[graph_result.memory.id] = graph_result
                else:
                    unique_expanded_memories[graph_result.memory.id].score += graph_result.score
                    
        expanded_memories = list(unique_expanded_memories.values())
        apply_time_dropoff, apply_access_dropoff = Retriever._get_dropoff_functions(
            [res.memory for res in expanded_memories]
        )
        for res in expanded_memories:
            res.score = apply_time_dropoff(res.score, res.memory)
            res.score = apply_access_dropoff(res.score, res.memory)
            res.is_context_expansion = True
            
        # sort by score
        expanded_memories.sort(key=lambda x: x.score, reverse=True)
        expanded_results = expanded_memories[:n_expanded]

            
        final_results = direct_results + expanded_results
        
        # print(f"Retriever: Retrieved {len(final_results)} memories ({len(direct_results)} direct, {len(expanded_results)} expanded).")
        
        return final_results
        

        
    @staticmethod
    def _get_dropoff_functions(memories: list[Memory], MAX_TIME_DROPOFF=0.2, MAX_ACCESS_DROPOFF=0.2)->tuple[callable, callable]:
        """
        Get dropoff functions for time and access feedback.
        
        Parameters
        ----------
        memories : list[Memory]
            The list of memories to analyze.
        MAX_TIME_DROPOFF : float
            The maximum dropoff factor for time-based decay.
        MAX_ACCESS_DROPOFF : float
            The maximum dropoff factor for access-based decay.
            
        Returns
        -------
        tuple[callable, callable]
            A tuple containing the time dropoff function and access dropoff function.\n
            apply_time_dropoff(score: float, memory: Memory) -> float\n
            apply_access_dropoff(score: float, memory: Memory) -> float"""
        
        oldest_memory_time = 4102488000
        newest_memory_time = 0
        most_positive_accesses = 0
        most_negative_accesses = 0
        most_total_accesses = 0
        
        for memory in memories:
            if memory.last_access is not None:
                if memory.last_access < oldest_memory_time:
                    oldest_memory_time = memory.last_access
                if memory.last_access > newest_memory_time:
                    newest_memory_time = memory.last_access
            if memory.total_access_count > most_total_accesses:
                most_total_accesses = memory.total_access_count
            if memory.positive_access_count > most_positive_accesses:
                most_positive_accesses = memory.positive_access_count
            if memory.negative_access_count > most_negative_accesses:
                most_negative_accesses = memory.negative_access_count
        
        # priotize more recent memories, drop off older ones
        def apply_time_dropoff(score: float, memory: Memory) -> float:
            memory_time = memory.last_access if memory.last_access is not None else oldest_memory_time
            if newest_memory_time <= oldest_memory_time:
                return score
            time_ratio = (memory_time - oldest_memory_time) / (newest_memory_time - oldest_memory_time)
            time_scale = MAX_TIME_DROPOFF + (1 - MAX_TIME_DROPOFF) * time_ratio
            return score * time_scale
        
        # priotize memories with more positive feedback, drop off those with more negative feedback
        # also, even if ration is 100% positive, we still want to prioritize ones with more total accesses
        # maybe do this 70:30 between positive ratio and total accesses
        def apply_access_dropoff(score: float, memory: Memory) -> float:
            if most_total_accesses == 0:
                return score
            pos_ratio = 0.0
            if memory.positive_access_count + memory.negative_access_count > 0:
                pos_ratio = memory.positive_access_count / (memory.positive_access_count + memory.negative_access_count)
            access_ratio = memory.total_access_count / most_total_accesses
            access_scale = MAX_ACCESS_DROPOFF + (1 - MAX_ACCESS_DROPOFF) * (0.7 * pos_ratio + 0.3 * access_ratio)
            return score * access_scale
        
        return apply_time_dropoff, apply_access_dropoff
        

    
    def _rank_memories(self, memories: list[Memory], entities: dict[str, int], embeddings: list[list[float]]) -> list[GraphResult]:
        # step by step pick best memories based on:
        # 1. embedding similarity (if embeddings are present)
        # 2. entity matches (weighted by entity weights)
        # 3. even distribution of memories (avoid too many from same source/time)
        
        # iteratively pick best memory until list is empty
        ranked_memories = []
        memory_pool = memories.copy()
        similarity_cache = {}
        EMBEDDING_MULTIPLIER = max(1, len(entities) * 0.75) #/ 2.0  # weight embeddings more if there are few entity matches
        # multiplier doesn't matter that much, since we apply falloff anyway
        EMBEDDING_MIN_SIMILARITY = 0.70
        FALLOFF_RATE = 0.8
        calc_falloff = lambda n: FALLOFF_RATE ** n  # exponential falloff based on n uses
        entity_counts = {entity: 0 for entity in entities} # n of times entity has been used in ranked memories
        embedding_sum = [0 for _ in embeddings] # sum of similarities in ranked memories
        
        apply_time_dropoff, apply_access_dropoff = Retriever._get_dropoff_functions(memories)
        
        while len(memory_pool) > 0:
            best_memory = None
            best_score = -99999999.0
            for memory in memory_pool:
                score = 0.0
                # embedding similarity
                if len(embeddings) > 0 and memory.embedding is not None:
                    for i, query_embedding in enumerate(embeddings):
                        key = f"{memory.id}_{i}"
                        if key in similarity_cache:
                            similarity = similarity_cache[key]
                        else:
                            similarity = (cosine_similarity(memory.embedding, query_embedding) + 1) / 2  # normalize to 0-1 
                            similarity_cache[key] = similarity
                        if similarity >= EMBEDDING_MIN_SIMILARITY:
                            # apply falloff based on how many times we've used this embedding
                            falloff = calc_falloff(embedding_sum[i])
                            score += similarity * EMBEDDING_MULTIPLIER * falloff
                        
                        
                # entity matches
                for entity in entities:
                    if entity in memory.entities:
                        # apply falloff based on how many times we've used this entity
                        falloff = calc_falloff(entity_counts[entity])
                        score += entities[entity] * falloff
                        
                # time decay
                if memory.last_access is not None:
                    score = apply_time_dropoff(score, memory)
                    
                # access feedback decay
                score = apply_access_dropoff(score, memory)
                
                if score > best_score:
                    best_score = score
                    best_memory = memory
            if best_memory is None:
                logging.error("No best memory found during ranking, this should not happen.")
                break
            ranked_memories.append(MemoryResult(memory=best_memory, score=best_score))
            memory_pool.remove(best_memory)
            # update entity counts
            for entity in entities:
                if entity in best_memory.entities:
                    entity_counts[entity] += 1
                    
            if best_memory.embedding is not None:
                for i, query_embedding in enumerate(embeddings):
                    similarity = similarity_cache[f"{best_memory.id}_{i}"]
                    if similarity >= EMBEDDING_MIN_SIMILARITY:
                        embedding_sum[i] += similarity**2
                        
        return ranked_memories
        
    
    def _collect_queries(self, query: RetrievalStep)-> tuple[dict[str, int], list[list[float]]]:
        entities = {}
        embeddings = []
        for q in query.queries:
            if q.embedding_query:
                embeddings.append(q.embedding_query.embedding.tolist())
            if q.entity_queries:
                for entity_query in q.entity_queries:
                    if not entity_query.entity in entities:
                        entities[entity_query.entity] = 0
                    entities[entity_query.entity] += entity_query.weight
                    
        return entities, embeddings

    def _calculate_entity_scores(self, conversation):
        assistant_entities = {}
        age = 1
        MAX_AGE = 20
        for msg in reversed(conversation.messages):
            if msg.role != Role.ASSISTANT:
                continue
            for entity, count in msg.entities.items():
                if not entity in assistant_entities:
                    assistant_entities[entity] = {'count': 0, 'age': age}
                assistant_entities[entity]['count'] += count
            age += 1
            if age > MAX_AGE:
                break
            
        # calculate scaling factors between 0 and 1, based on count and age
        # basically, more recent and more mentioned entities should reduce the weight more
        # LOWEST_SCALE = 0.3 # basically what we want to get for very common/recent entities
        # HIGHEST_SCALE = 1.0 # for entities not mentioned before
        # something with age 10 and count 1 should be around 0.8
        def scale_formula(count, age):
            age_factor = 1 - math.exp(-age / 5) # decays with age
            # scale up with count, but with diminishing returns
            #scale = (age_factor / math.exp(1-(1/count))) * (1-age_factor**5) + age_factor**6
            scale = age_factor - age_factor / 15 * min(count-1, 10) * (1 - age_factor**4)
            return scale
        
        scales = {}
        for entity in assistant_entities:
            data = assistant_entities[entity]
            scales[entity] = scale_formula(data['count'], data['age'])

        return scales
    
    def give_query_feedback(self, step: RetrievalStep, evaluation: Evaluation)-> bool:
        """Provide feedback on a retrieval step based on its evaluation.
        
        Parameters
        ----------
        step : RetrievalStep
            The retrieval step that was evaluated.
        evaluation : Evaluation
            The evaluation from the Evaluator.
        
        Returns
        -------
        bool
            True if the feedback was processed successfully, False otherwise.
        """
        query_entities = set() # collection of entities that were used in the query
        for q in step.queries:
            if q.entity_queries:
                for entity_query in q.entity_queries:
                    query_entities.add(entity_query.entity)
                    
        feedbacks = []
        for memory, relevance, entities in zip(evaluation.memories, evaluation.ratings, evaluation.memory_keywords):
            # collect union of query entities and memory entities, then add any keywords from evaluation
            related_entities = set(memory.entities.keys())
            
            if relevance in [MemoryRelevance.PERFECT, MemoryRelevance.RELEVANT, MemoryRelevance.SUPPORTING]:
                # only add these if memory was relevant, since we don't want to negatively reinforce actual relevant entities
                related_entities = related_entities.union(entities)
                    
            if relevance in [MemoryRelevance.PERFECT, MemoryRelevance.RELEVANT, MemoryRelevance.IRRELEVANT, MemoryRelevance.NONSENSE]:
                # only add these if memory was evaluated as relevant or irrelevant
                # if it was supporting, it might not be directly related to the query
                related_entities = related_entities.union(query_entities)
                
            feedback = FeedbackType.NEUTRAL
            # map relevance to feedback type
            match relevance:
                case MemoryRelevance.PERFECT | MemoryRelevance.RELEVANT | MemoryRelevance.SUPPORTING:
                    feedback = FeedbackType.POSITIVE
                case MemoryRelevance.IRRELEVANT | MemoryRelevance.NONSENSE:
                    feedback = FeedbackType.NEGATIVE
                case _:
                    # for UNKNOWN or other cases, keep neutral
                    feedback = FeedbackType.NEUTRAL
            
            feedbacks.append(
                MemoryFeedback(
                    memory_id=memory.id,
                    feedback=feedback,
                    entities=list(related_entities)
                )
            )
            
        res = self.graph_memory.update_memory_weights(feedbacks)
        return res