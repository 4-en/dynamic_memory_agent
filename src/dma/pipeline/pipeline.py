from dma.core.conversation import Conversation
from dma.core.message import Message, Role
from dma.core.memory import Memory
from dma.core.retrieval import RetrievalStep, RetrievalQuery, Retrieval, MemoryResult
from dma.config import DmaConfig, get_config

from dma.query import QueryGenerator
from dma.generator import LowLevelLlamaCppGenerator
from dma.memory import Retriever
import os
from dotenv import load_dotenv
import logging
from typing import Callable
from dma.utils import NER
from .pipeline_status import PipelineStatus, PipelineUpdate


load_dotenv()

class Pipeline:
    """
    Pipeline class to represent a pipeline.
    This class controls the retrieval and generation process.
    This is a simple, linear pipeline that can use components passed to it to generate responses.
    It can be used to generate responses based on a conversation, and can be extended with additional components.
    

    The pipeline consists of an evaluator, a retriever, a generator, and a refiner.
    The retriever adds context to the conversation, which is then passed
    to the generator to generate a response. The refiner can be used to
    refine the response generated by the generator.
    
    At minimum, the pipeline requires a generator to generate responses, which would simply pass the conversation to the generator.

    Pipeline:
        Input: Conversation
            ↓
        Evaluator  (Decides whether and what to retrieve)
            ↓
        Retriever  (Fetches relevant information, if needed)
            ↓
        Generator  (LLM generates response)
            ↓
        Output: Message | None
    
    """

    def __init__(self) -> None:   
        """
        Initialize the pipeline.

        This simple pipeline consists of a retriever, generator, and refiner.
        The retriever adds context to the conversation, which is then passed
        to the generator to generate a response. The refiner can be used to
        refine the response generated by the generator.

        Pipeline:
            Input: Conversation
                ↓
            QueryGenerator  (Decides whether to retrieve or not)
                ↓
            Retriever  (Fetches relevant information, if needed)
                ↓
            Generator  (LLM generates response)
                ↓
            Output: Message | None


        """
        
        # load environment variables from .env file in root directory
        dotenv_path = "./.env"
        if not os.path.exists(dotenv_path):
            logging.info(f"Creating .env file in {os.path.abspath(dotenv_path)}")
            with open(dotenv_path, "w") as f:
                f.write("# Add environment variables here")
        else:
            load_dotenv(dotenv_path)
        
        # placeholders for components
        self.generator = LowLevelLlamaCppGenerator()
        self.query_generator = QueryGenerator(self.generator)
        self.retriever = Retriever()
        self.config = get_config()
        
        

            
        self._system_message = self._generate_system_message()

    def save(self) -> None:
        """
        Save the pipeline.
        This method can be used to save pipeline components to disk.
        """
        
        self.config.save(self.root_dir+"/config.txt")

    def add_memories(self, memories: list[Memory]) -> None:
        """
        Add memories to the memory handler.

        Parameters
        ----------
        memories : list[Memory]
            The memories to add.
        """
        if self.retriever:
            for memory in memories:
                self.retriever.add_memory(memory)
        else:
            logging.warning("No retriever set, cannot add memories.")

        
    def get_root_dir(self) -> str:
        """
        Get the root directory of the pipeline.

        Returns
        -------
        str
            The root directory of the pipeline.
            (Without trailing slash)
        """
        return os.path.abspath(self.root_dir)
    
    def get_data_dir(self) -> str:
        """
        Get the data directory of the pipeline.

        Returns
        -------
        str
            The data directory of the pipeline.
            (Without trailing slash)
        """
        return os.path.abspath(self.root_dir+"/data")
    
    def get_config(self) -> DmaConfig:
        """
        Get the configuration of the pipeline.

        Returns
        -------
        Config
            The configuration of the pipeline.
        """
        return self.config
    
    def get_config_dir(self) -> str:
        """
        Get the directory of configuration files.
        The main configuration file is usually stored in the root directory.

        Returns
        -------
        str
            The directory of the configuration file.
        """
        return os.path.abspath(self.root_dir+"/config")
        
    def _generate_system_message(self) -> Message:
        """
        Generates a system message as instruction for the model.
        This message is inserted as the first message of a conversation.

        Returns
        -------
        Message
            The system message.
        """
        
        instruction = self.config.llm_instruction
        if not instruction or instruction.strip() == "":
            instruction = "You are a helpful AI assistant. Please provide useful and grounded information to the user."
            
        if self.config.context_injection_method == "expert":
            instruction += "\nUWhen available, use the expert's knowledge to respond, without acknowledging the expert."
        elif self.config.context_injection_method == "reasoning":
            pass
        else:
            logging.warning(f"Unknown context injection method: {self.config.context_injection_method}.")

        # instruction += "\n\nOnly respond with direct speech. Don't include thoughts, inner monologue or actions in your response."
        
        return Message(instruction, Role.SYSTEM)
    
    
    def text(self, text: str) -> str | None:
        """
        Basic string interface that handles conversation state.

        Parameters
        ----------
        text : str
            The text content of the message.

        Returns
        -------
        str | None
            The message content or None if no response was generated.
        """
        if not hasattr(self, "_auto_conversation"):
            self._auto_conversation = Conversation()

        self._auto_conversation.add_message(Message(text, Role.USER))
        
        response = self.generate(self._auto_conversation)
        if response:
            self._auto_conversation.add_message(response)
            
        return response.message_text if response else None
    
    def _update_progress(
        self, 
        callback: Callable[[PipelineUpdate], None] | None,
        status: PipelineStatus,
        message: str | None = None,
        progress: float = 0.0,
        retrieval_step: RetrievalStep | None = None
        ) -> None:
        """
        Update the progress of the pipeline.

        Parameters
        ----------
        callback : Callable[[PipelineUpdate], None] | None
            The callback to call with the update.
        status : PipelineStatus
            The current status of the pipeline.
        message : str | None
            An optional message to include with the update.
        progress : float
            The progress of the pipeline (0.0 to 1.0).
        retrieval_step : RetrievalStep | None
            An optional retrieval step to include with the update.
        """
        if callback is not None:
            update = PipelineUpdate(
                status=status,
                message=message,
                progress=progress,
                retrieval_step=retrieval_step
            )
            try:
                # since this is a user provided callback, we need to catch any exceptions
                callback(update)
            except Exception as e:
                logging.error(f"Error during progress update: {e}")

    def _generate(
        self, 
        conversation: Conversation, 
        progress_callback: Callable[[PipelineUpdate], None] | None = None,
        *args, 
        **kwargs
        ) -> Message | None:
        """
        Generate a response based on the conversation.

        Parameters
        ----------
        conversation : Conversation
            The conversation to generate a response from.
        progress_callback : Callable[[PipelineUpdate], None] | None
            A callback to call with progress updates.
        args
            Additional arguments.
        kwargs
            Additional keyword arguments.

        Returns
        -------
        Message | None
            The generated response.
            If no response was generated, return None.
            This can happen if the pipeline is setup to decide 
            whether to generate a response or not, for example if
            the message wasn't directed at the AI.
            The generated response if no
        """
        
        total_steps = 1 # one for final generation
        if self.config.max_retrieval_iterations > 0 and self.config.enable_retrieval:
            # *2 since each retrieval iteration has a query generation and retrieval step
            # +1 for final summary generation
            total_steps += self.config.max_retrieval_iterations * 2 + 1
        if self.config.enable_pre_retrieval:
            total_steps += 2
            
        current_step = 0
        
        # sanity check: we should have at least one message in the conversation
        # (the users first prompt)
        if len(conversation.messages) == 0:
            raise ValueError("Conversation must have at least one message.")
        
        if conversation.messages[-1].role != Role.USER:
            raise ValueError("The last message in the conversation must be from the user.")
        
        # in case the conversation was not a request from an api and is the same object used to
        # store the actual conversation history, we need to copy it to avoid modifying the original
        # with things like instructions, system messages, etc.
        conversation = conversation.copy()
        
        # TODO: enable this later
        # conversation = self._limit_input_length(conversation)
        
        # create Retrieval object to store retrieval steps
        retrieval = Retrieval(
            conversation=conversation,
            user_prompt=conversation.messages[-1],
            max_iterations=self.config.max_retrieval_iterations
        )

        # prefetch entities from prompt using NER
        prompt: Message = conversation.messages[-1]
        if prompt.message_text and prompt.role == Role.USER and self.config.enable_pre_retrieval:
            self._update_progress(
                progress_callback,
                PipelineStatus.QUERY_GENERATION,
                "Extracting entities from user prompt for pre-retrieval...",
                current_step / total_steps
            )
            entities = NER.get_entities(prompt.message_text)
            current_step += 1
            
            if len(entities) > 0:
                logging.debug(f"Found entities in prompt: {entities}")
                prompt.entities = entities
                pre_query = RetrievalQuery.from_entities(entities, weight=1.0)
                pre_step = RetrievalStep(queries=[pre_query], is_pre_query=True)
                retrieval.add_step(pre_step)
                # since this is the pre-query, we don't count it against the max iterations
                retrieval.current_iteration -= 1
                
                self._update_progress(
                    progress_callback,
                    PipelineStatus.RETRIEVAL,
                    "Performing pre-retrieval based on extracted entities...",
                    current_step / total_steps,
                    retrieval_step=pre_step
                )

                self.retriever.retrieve(pre_query, top_k=self.config.retrieval_num_results)
                
                current_step += 1
                
                self._update_progress(
                    progress_callback,
                    PipelineStatus.RETRIEVAL_UPDATE,
                    "Pre-retrieval completed.",
                    current_step / total_steps,
                    retrieval_step=pre_step
                )
            else:
                current_step += 1
                logging.debug(f"No entities found in prompt.")
                

        self.retrieval_loop(conversation, retrieval, progress_callback, current_step, total_steps)
        current_step += self.config.max_retrieval_iterations * 2

                
        # TODO: generate and add memories after retrieval
        # after retrieval so we can make memories as verbose and context aware as possible, which
        # will help with future retrieval
        
        
        # insert the system message as the first message
        conversation.messages.insert(0, self._system_message)
        
        final_summary = retrieval.finalize()
        if not final_summary or final_summary.strip() == "":
            final_summary = None
            
        current_step += 1
        self._update_progress(
            progress_callback,
            PipelineStatus.RESPONSE_GENERATION,
            "Generating final response...",
            current_step / total_steps
        )

        # Generate response
        # print(f"Request: {conversation}")
        response = self.generator.generate(conversation, context=final_summary)


        # TODO: add refiner step here
        # some cleanup for common issues with generated responses
        # remove quotation marks around the response if they are the first and last characters
        marks = ['"', "'"]
        if response and response.message_text and response.message_text[0] in marks and response.message_text[-1] in marks:
            response.message_text = response.message_text[1:-1]
            
        self._update_progress(
            progress_callback,
            PipelineStatus.COMPLETED,
            "Response generation completed.",
            1.0
        )
        
        return response
    
    def generate(
        self, 
        conversation: Conversation, 
        progress_callback: Callable[[PipelineUpdate], None] | None = None,
        *args, 
        **kwargs
        ) -> Message | None:
        """
        Generate a response based on the conversation.

        Parameters
        ----------
        conversation : Conversation
            The conversation to generate a response from.
        progress_callback : Callable[[PipelineUpdate], None] | None
            A callback to call with progress updates.
        args
            Additional arguments.
        kwargs
            Additional keyword arguments.

        Returns
        -------
        Message | None
            The generated response.
            If no response was generated, return None.
            This can happen if the pipeline is setup to decide 
            whether to generate a response or not, for example if
            the message wasn't directed at the AI.
            The generated response if no
        """
        
        try:
            return self._generate(conversation, progress_callback, *args, **kwargs)
        except Exception as e:
            logging.error(f"Error during generation: {e}")
            self._update_progress(
                progress_callback,
                PipelineStatus.ERROR,
                f"Error during generation: {e}",
                1.0
            )
            raise e
    
    def _get_test_memories(self) -> list[Memory]:
        """
        Get test memories for testing purposes.

        Returns
        -------
        list[Memory]
            The test memories.
        """

        memories = []
        sample_texts = [
            "The James Webb Space Telescope (JWST) has a primary mirror with a diameter of 6.5 meters (21.3 feet), which gives it a collecting area of approximately 25.4 square meters. This significantly larger size compared to previous telescopes allows it to gather more light, enabling it to see objects that are older, more distant, or fainter.",
            "Unlike Hubble's single, monolithic mirror, the JWST's primary mirror is composed of 18 individual hexagonal segments. Each segment is made of beryllium, which is both strong and lightweight, and is coated with a microscopically thin layer of gold to optimize its reflectivity for infrared light. These segments had to be folded to fit inside the rocket fairing for launch and were later unfolded in space.",
            "The Hubble Space Telescope (HST) utilizes a primary mirror that is 2.4 meters (7.9 feet) in diameter, with a collecting area of about 4.5 square meters. It is a single-piece, polished glass mirror coated with layers of aluminum and magnesium fluoride. This design is optimized for observing in the near-ultraviolet, visible, and near-infrared spectra.",
            "Hubble's mirror is a monolithic structure, meaning it is made from a single piece of glass. This design choice provides high optical quality and stability, which is essential for the precise observations Hubble conducts. The mirror's surface was polished to an accuracy of about 10 nanometers, allowing it to capture sharp images of distant celestial objects."
        ]

        for i, text in enumerate(sample_texts):
            memory = Memory(
                memory=text,
                source=f"Test Memory {i+1}",
                entities=["JWST", "Hubble Space Telescope", "mirror", "infrared light"]
            )
            memories.append(memory)

        return memories
    
    def retrieval_loop(self, conversation: Conversation, retrieval: Retrieval, progress_callback: Callable[[PipelineUpdate], None] | None, current_step: int, total_steps: int) -> None:
        # TODO: consider adding a dedicated mechanism to decide whether to evaluate or not
        # could be sentence transformer based classifier that decides whether to evaluate or not
        # could be trained using saved conversations and whether they led to retrieval or not
        # for now, we will just evaluate every time and stop when no queries are generated
        # generate queries
        
        if (not self.query_generator) or (not self.retriever):
            logging.warning("No query generator or retriever set, skipping retrieval.")
            return
            
        # TODO: add retriever and retrieval loop
        while not retrieval.done:
            logging.debug(f"Retrieval iteration {retrieval.current_iteration+1}/{retrieval.max_iterations}")
            
            self._update_progress(
                progress_callback,
                PipelineStatus.QUERY_GENERATION,
                "Generating retrieval queries...",
                current_step / total_steps
            )
            retrieval = self.query_generator.generate_queries(conversation, retrieval)
            current_step += 1
            
            self._update_progress(
                progress_callback,
                PipelineStatus.QUERY_UPDATE,
                "Retrieval queries generated.",
                current_step / total_steps,
                retrieval_step=retrieval.steps[-1]
            )
            
            self._update_progress(
                progress_callback,
                PipelineStatus.RETRIEVAL,
                "Performing retrieval...",
                current_step / total_steps,
                retrieval_step=retrieval.steps[-1]
            )
            
            last_step = retrieval.steps[-1]
            if len(last_step.queries) == 0 or len(last_step.results) > 0:
                logging.debug("No queries generated or results found, stopping retrieval.")
                retrieval.done = True
                return

            results: list[MemoryResult] = self.retriever.retrieve(last_step, top_k=self.config.retrieval_num_results)
            # TODO: what to do if no results found?
            # for now, we just add an empty result and mark as done
            # alternatives:
            # - continue with next iteration and different queries
            # - ask user for clarification/more info
            # - use web search to find more info (and create memories from that, allowing for future retrieval)
            last_step.results.extend(results)
            if len(results) == 0:
                logging.debug("No results found, stopping retrieval.")
                retrieval.done = True
            
            current_step += 1
            self._update_progress(
                progress_callback,
                PipelineStatus.RETRIEVAL_UPDATE,
                "Retrieval completed.",
                current_step / total_steps,
                retrieval_step=last_step
            )
            
            # TODO: add step summary generation here?

        return
    
    def _append_summary(self, conversation: Conversation, summary: str):
        """
        Append a summary of the retrieved information to the conversation.

        Parameters
        ----------
        conversation : Conversation
            The conversation to append the summary to.
        summary : str
            The summary of the retrieved information to append to the conversation.
        """
        last_message = conversation.messages[-1]
        conversation.messages = conversation.messages[:]

        summary = f"{summary}\nI should probably reply to {last_message.user.name}... What was said again...?"
        message = Message(summary, Role.ASSISTANT)
        # insert the summary before the actual question
        # otherwise, some models might not be able to attend to the actual question
        # or interpret the summary as the message to respond to
        conversation.messages.append(message)
        conversation.messages.append(last_message)

    
    def _limit_input_length(self, conversation: Conversation) -> Conversation:
        """
        Limit the input length of the conversation.
        To limit api/computing costs, we limit the input length of the conversation.
        To take advantage of caching (specifically for the OpenAI API), we will try to
        keep the beginning of the sequence as consistent as possible.

        For example, if the limit is 4096 tokens, we will not cut off at the exact token, but rather
        cut off a fixed number of tokens from the start of the conversation once the limit is reached.
        This allows us to keep the beginning of the conversation consistent between multiple requests,
        even if the end of the conversation changes every time.

        Another option would be to use message ids to keep track of the last message that was included
        and cache last conversations sent to the API.
        
        Example:
        Limit: 10 tokens
        - Seq 1: "A B C D E F G" (Fine, 7 tokens)
        - Seq 2: "A B C D E F G H I J (Fine, 10 tokens)
        - Seq 3: "A B C D E F G H I J K L" (Limit reached, cut off 5 tokens (until <= 10 tokens)) -> "F G H I J K L"
        - Seq 4: "A B C D E F G H I J K L M N" (Limit reached, cut off 5 tokens (until <= 10 tokens)) -> "F G H I J K L M N"
        The start of sequence 3 and 4 is consistent, while the end changes. The "F G H I J K L" can be cached
        and reduce cost when sending the next request.

        Note: This only works if the request contains the full conversation history withouth manually
        shortening it. If the conversation is manually shortened, this method will not work as intended.

        Parameters
        ----------
        conversation : Conversation
            The conversation to limit the input length of.

        Returns
        -------
        Conversation
            The conversation with the input length limited.
        """

        total_tokens = conversation.estimate_tokens()
        max_tokens = self.config.max_input_length
        cut_off_window = self.config.cut_off_window

        if total_tokens <= max_tokens or max_tokens <= 0:
            return conversation
        

        # some checks to avoid annoying debugging
        if max_tokens < cut_off_window:
            raise ValueError("cut_off_window must be less than or equal to max_input_length")
        
        if max_tokens < 128:
            raise ValueError("max_input_length must be at least 128 tokens")
        
        target_tokens = max_tokens - cut_off_window

        # cut off messages from beginning until total tokens <= target tokens
        while total_tokens > target_tokens:
            # remove first message
            message = conversation.messages.pop(0)
            total_tokens -= message.estimate_tokens()

        return conversation
    
    def __call__(self, input: Conversation | str, *args, **kwargs) -> Message | None:
        """
        Call the pipeline with a conversation or a string input.
        If a string is passed, the conversation state will be handled automatically.
        Otherwise, the conversation will be used as is.
        
        Parameters
        ----------
        input : Conversation | str
            The input to the pipeline. Can be a conversation or a string.
        args
            Additional arguments.
        kwargs
            Additional keyword arguments.
            These will be passed to the underlying model.
            
        Returns
        -------
        Message | None
            The generated response or None if no response was generated.
            If a string is passed, the response will be based on the conversation state.
            If a conversation is passed, the response will be based on the conversation.
        """
        if isinstance(input, Conversation):
            return self.generate(input, *args, **kwargs)
        elif isinstance(input, str):
            return self.text(input)
        else:
            raise ValueError("Input must be a Conversation or a string.")