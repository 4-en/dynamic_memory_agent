from dma.core import Conversation, Message, Role, Memory
from dma.config import DmaConfig, get_config

#from dma.evaluator import BaseEvaluator
from dma.generator import LlamaCppChatCompletionGenerator
#from dma.retriever import BaseRetriever
#from dma.utils import get_storage_location
import os
from dotenv import load_dotenv
import logging


load_dotenv()

class Pipeline:
    """
    Pipeline class to represent a pipeline.
    This class controls the retrieval and generation process.
    This is a simple, linear pipeline that can use components passed to it to generate responses.
    It can be used to generate responses based on a conversation, and can be extended with additional components.
    

    The pipeline consists of an evaluator, a retriever, a generator, and a refiner.
    The retriever adds context to the conversation, which is then passed
    to the generator to generate a response. The refiner can be used to
    refine the response generated by the generator.
    
    At minimum, the pipeline requires a generator to generate responses, which would simply pass the conversation to the generator.

    Pipeline:
        Input: Conversation
            ↓
        Evaluator  (Decides whether and what to retrieve)
            ↓
        Retriever  (Fetches relevant information, if needed)
            ↓
        Generator  (LLM generates response)
            ↓
        Output: Message | None
    
    """

    def __init__(self) -> None:   
        """
        Initialize the pipeline.

        This simple pipeline consists of a retriever, generator, and refiner.
        The retriever adds context to the conversation, which is then passed
        to the generator to generate a response. The refiner can be used to
        refine the response generated by the generator.

        Pipeline:
            Input: Conversation
                ↓
            Evaluator  (Decides whether to retrieve or not)
                ↓
            Retriever  (Fetches relevant information, if needed)
                ↓
            Generator  (LLM generates response)
                ↓
            Output: Message | None


        """
        
        # load environment variables from .env file in root directory
        dotenv_path = "./.env"
        if not os.path.exists(dotenv_path):
            print(f"Creating .env file in {os.path.abspath(dotenv_path)}")
            with open(dotenv_path, "w") as f:
                f.write("# Add environment variables here")
        else:
            load_dotenv(dotenv_path)
        
        # placeholders for components
        self.generator = LlamaCppChatCompletionGenerator()
        self.evaluator = None
        self.retriever = None
        self.config = get_config()
        
        

            
        self._system_message = self._generate_system_message()

    def save(self) -> None:
        """
        Save the pipeline.
        This method can be used to save pipeline components to disk.
        """
        
        self.config.save(self.root_dir+"/config.txt")

    def add_memories(self, memories: list[Memory]) -> None:
        """
        Add memories to the memory handler.

        Parameters
        ----------
        memories : list[Memory]
            The memories to add.
        """
        if self.memory_handler:
            for memory in memories:
                memory_str = memory.memory
                print(f"Adding memory: {memory_str}")
                print(f"Person: {memory.entities}, Relevance: {memory.time_relevance}, Time: {memory.memory_time_point}, Truthfulness: {memory.truthfulness}")
                self.memory_handler.add_memory(memory)

        
    def get_root_dir(self) -> str:
        """
        Get the root directory of the pipeline.

        Returns
        -------
        str
            The root directory of the pipeline.
            (Without trailing slash)
        """
        return os.path.abspath(self.root_dir)
    
    def get_data_dir(self) -> str:
        """
        Get the data directory of the pipeline.

        Returns
        -------
        str
            The data directory of the pipeline.
            (Without trailing slash)
        """
        return os.path.abspath(self.root_dir+"/data")
    
    def get_config(self) -> DmaConfig:
        """
        Get the configuration of the pipeline.

        Returns
        -------
        Config
            The configuration of the pipeline.
        """
        return self.config
    
    def get_config_dir(self) -> str:
        """
        Get the directory of configuration files.
        The main configuration file is usually stored in the root directory.

        Returns
        -------
        str
            The directory of the configuration file.
        """
        return os.path.abspath(self.root_dir+"/config")
        
    def _generate_system_message(self) -> Message:
        """
        Generates a system message as instruction for the model.
        This message is inserted as the first message of a conversation.

        Returns
        -------
        Message
            The system message.
        """
        
        instruction = self.config.llm_instruction
        if not instruction or instruction.strip() == "":
            instruction = "You are a helpful AI assistant. Please provide useful and grounded information to the user."
            
        if self.config.context_injection_method == "expert":
            instruction += "\nUWhen available, use the expert's knowledge to resond, without acknowledging the expert."
        elif self.config.context_injection_method == "reasoning":
            pass
        else:
            logging.warning(f"Unknown context injection method: {self.config.context_injection_method}.")

        # instruction += "\n\nOnly respond with direct speech. Don't include thoughts, inner monologue or actions in your response."
        
        return Message(instruction, Role.SYSTEM)
    
    
    def text(self, text: str) -> str | None:
        """
        Basic string interface that handles conversation state.

        Parameters
        ----------
        text : str
            The text content of the message.

        Returns
        -------
        str | None
            The message content or None if no response was generated.
        """
        if not hasattr(self, "_auto_conversation"):
            self._auto_conversation = Conversation()

        self._auto_conversation.add_message(Message(text, Role.USER))
        
        response = self.generate(self._auto_conversation)
        if response:
            self._auto_conversation.add_message(response)
            
        return response.message_text if response else None

    def generate(self, conversation: Conversation, *args, **kwargs) -> Message | None:
        """
        Generate a response based on the conversation.

        Parameters
        ----------
        conversation : Conversation
            The conversation to generate a response from.
        args
            Additional arguments.
        kwargs
            Additional keyword arguments.

        Returns
        -------
        Message | None
            The generated response.
            If no response was generated, return None.
            This can happen if the pipeline is setup to decide 
            whether to generate a response or not, for example if
            the message wasn't directed at the AI.
            The generated response if no
        """
        
        # in case the conversation was not a request from an api and is the same object used to
        # store the actual conversation history, we need to copy it to avoid modifying the original
        # with things like instructions, system messages, etc.
        conversation = conversation.copy()
        
        # TODO: enable this later
        # conversation = self._limit_input_length(conversation)
        
        # Evaluate the conversation
        queries = []
        evaluator_context = []
        memories = []
        if self.evaluator:
            try:
                evaluation = self.evaluator.evaluate(conversation)
                queries = evaluation.queries
                evaluator_context = evaluation.context
                memories = evaluation.memories
                reply_expectation = evaluation.reply_expectation
                if reply_expectation <= 0.3:
                    # TODO: NOTE: This is kinda arbitrary, maybe a better way to handle this
                    # would be to have a bunch of yes/no questions about the conversation
                    # state and calculate a score based on that.
                    
                    # also take into consideration meta data of conversation, like users, timestamps, private/group chat, etc.
                    return None
            except Exception as e:
                print(f"Failed to evaluate conversation: {e}")
                # continue without queries

            
        
        # Retrieve information
        if len(queries) > 0 and self.retriever:
            # queries = [query for query in queries if query and query.query_type != "PERSONAL"]
            # TODO: make better use of meta data
            # print(f"Retrieving information for queries: {queries}")

            for query in queries:
                print(f"Query: {query.query}, Type: {query.query_type}, {query.time_relevance}")

            retrieved_info = self.retriever.retrieve(conversation, queries)
            retrieved_info.purge(min_score=0.6, max_results=3)
            if len(retrieved_info) > 0:
                # self._append_retrieval_results(conversation, retrieved_info)
                user_message = conversation.messages[-1]
                user_message_content = f"I remember {user_message.user.name} said:\n\"{user_message.message_text}\""
                if len(evaluator_context) > 0:
                    user_message_content = f"{user_message_content}\n\n{evaluator_context[0]}"
                summary = self.evaluator.summarize_retrieval(retrieved_info, user_message_content)
                print(f"Retrieved information: {summary}")
                self._append_summary(conversation, summary)

                # TODO:NOTE: consider adding all "thoughts" based on retrieved information to character memories
                # for example, if the AI thinks about something, it should be added to the memory
                # this way, the AI can remember what it thought about and why it thought that way
                # it could improve the AI's retrieval of information and reasoning. Kinda like learning from experience.
                # Possible downside: This could lead to the AI remembering a lot of irrelevant information or creating
                # duplicate memories. Maybe a way to filter out irrelevant or duplicate memories?
                # Also a way to "forget" memories that are no longer relevant or useful or proven wrong.
                # To remove false memories, we'd need a system to find contradictions in memories. Then we could decide
                # which memory is more likely to be true based on other memories or external information (generate queries and search web).
                # The hard part is probably finding contradictions in memories. Just similarity search might work, but not sure how well...
            
            else:
                print(f"No relevant information found.")

                
        # add memories after retrieval
        if len(memories) > 0:
            self.add_memories(memories)
        
        # insert the system message as the first message
        conversation.messages.insert(0, self._system_message)
        
        
        # Generate response
        # print(f"Request: {conversation}")
        response = self.generator.generate(conversation)


        # some cleanup for common issues with generated responses
        # remove quotation marks around the response if they are the first and last characters
        marks = ['"', "'"]
        if response and response.message_text and response.message_text[0] in marks and response.message_text[-1] in marks:
            response.message_text = response.message_text[1:-1]
        
        return response
    
    def _append_summary(self, conversation: Conversation, summary: str):
        """
        Append a summary of the retrieved information to the conversation.

        Parameters
        ----------
        conversation : Conversation
            The conversation to append the summary to.
        summary : str
            The summary of the retrieved information to append to the conversation.
        """
        last_message = conversation.messages[-1]
        conversation.messages = conversation.messages[:]

        summary = f"{summary}\nI should probably reply to {last_message.user.name}... What was said again...?"
        message = Message(summary, Role.ASSISTANT)
        # insert the summary before the actual question
        # otherwise, some models might not be able to attend to the actual question
        # or interpret the summary as the message to respond to
        conversation.messages.append(message)
        conversation.messages.append(last_message)

    
    def _append_retrieval_results(self, conversation: Conversation, retrieved_info):
        """
        Append the retrieved information to the conversation.

        Parameters
        ----------
        conversation : Conversation
            The conversation to append the retrieved information to.
        retrieved_info : RetrievalResults
            The retrieved information to append to the conversation.
        """
        # creates new messages in the conversation, for example:
        # <INNER MONOLOGUE> The capital of France is Paris. <-- retrieved information
        # <USER> What is the capital of France?
        # <ASSISTANT> (Answer based on retrieved information)
        # <USER> Thanks!
        # We insert the retrieved information before the last message in the conversation,
        # to make it easier for the model to attend to the actual question asked by the user.
        # TODO: Consider adding user question before AND after the retrieved information? This might help the model to extract the correct context.

        last_message = conversation.messages[-1]
        conversation.messages = conversation.messages[:-1]
        
        query_results = retrieved_info.top_k(3)

        for result in reversed(query_results): # reversed so best results are last
            query_result = result.result
            if query_result and len(query_result) > 10:
                print(f"Appending query result: {query_result[:100]}... Score: {result.score}")
                message = Message(query_result, User("INNER_MONOLOGUE", Role.USER))
                conversation.messages.append(message)

        conversation.messages.append(last_message)
    
    def _limit_input_length(self, conversation: Conversation) -> Conversation:
        """
        Limit the input length of the conversation.
        To limit api/computing costs, we limit the input length of the conversation.
        To take advantage of caching (specifically for the OpenAI API), we will try to
        keep the beginning of the sequence as consistent as possible.

        For example, if the limit is 4096 tokens, we will not cut off at the exact token, but rather
        cut off a fixed number of tokens from the start of the conversation once the limit is reached.
        This allows us to keep the beginning of the conversation consistent between multiple requests,
        even if the end of the conversation changes every time.

        Another option would be to use message ids to keep track of the last message that was included
        and cache last conversations sent to the API.
        
        Example:
        Limit: 10 tokens
        - Seq 1: "A B C D E F G" (Fine, 7 tokens)
        - Seq 2: "A B C D E F G H I J (Fine, 10 tokens)
        - Seq 3: "A B C D E F G H I J K L" (Limit reached, cut off 5 tokens (until <= 10 tokens)) -> "F G H I J K L"
        - Seq 4: "A B C D E F G H I J K L M N" (Limit reached, cut off 5 tokens (until <= 10 tokens)) -> "F G H I J K L M N"
        The start of sequence 3 and 4 is consistent, while the end changes. The "F G H I J K L" can be cached
        and reduce cost when sending the next request.

        Note: This only works if the request contains the full conversation history withouth manually
        shortening it. If the conversation is manually shortened, this method will not work as intended.

        Parameters
        ----------
        conversation : Conversation
            The conversation to limit the input length of.

        Returns
        -------
        Conversation
            The conversation with the input length limited.
        """

        total_tokens = conversation.estimate_tokens()
        max_tokens = self.config.max_input_length
        cut_off_window = self.config.cut_off_window

        if total_tokens <= max_tokens or max_tokens <= 0:
            return conversation
        

        # some checks to avoid annoying debugging
        if max_tokens < cut_off_window:
            raise ValueError("cut_off_window must be less than or equal to max_input_length")
        
        if max_tokens < 128:
            raise ValueError("max_input_length must be at least 128 tokens")
        
        target_tokens = max_tokens - cut_off_window

        # cut off messages from beginning until total tokens <= target tokens
        while total_tokens > target_tokens:
            # remove first message
            message = conversation.messages.pop(0)
            total_tokens -= message.estimate_tokens()

        return conversation
    
    def __call__(self, input: Conversation | str, *args, **kwargs) -> Message | None:
        """
        Call the pipeline with a conversation or a string input.
        If a string is passed, the conversation state will be handled automatically.
        Otherwise, the conversation will be used as is.
        
        Parameters
        ----------
        input : Conversation | str
            The input to the pipeline. Can be a conversation or a string.
        args
            Additional arguments.
        kwargs
            Additional keyword arguments.
            These will be passed to the underlying model.
            
        Returns
        -------
        Message | None
            The generated response or None if no response was generated.
            If a string is passed, the response will be based on the conversation state.
            If a conversation is passed, the response will be based on the conversation.
        """
        if isinstance(input, Conversation):
            return self.generate(input, *args, **kwargs)
        elif isinstance(input, str):
            return self.text(input)
        else:
            raise ValueError("Input must be a Conversation or a string.")